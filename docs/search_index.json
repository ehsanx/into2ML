[["index.html", "Understanding the Basics and Usage of Machine Learning in Medical Literature Preface Goal Philosophy Pre-requisites Key References Version History Contributor List License", " Understanding the Basics and Usage of Machine Learning in Medical Literature Ehsan Karim 2021-11-22 Preface This document is a very basic introduction to machine learning for Medicine. Goal Part I: Basics Basic Machine Learning Terminologies (pre-reading) Supervised vs. Unsupervised Learning Algorithms Concepts Examples Over-Fitting, Measuring Performance and Model Tuning Overall steps Part II: Application in Medical Literature Model development Model validation Clinical implementation Philosophy Code-first philosophy is adopted for this tutorial; demonstrating the analyses through one real data analysis problem used in the literature. This tutorial is not theory-focused, nor utilizes simulated data to explain the ideas. Given the focus on implementation, theory is beyond the scope of this tutorial. Pre-requisites The prerequisites are knowledge of multiple regression analysis and basic probability. Software demonstrations and codes will be provided in R, although proficiency in R is not required for understanding the concepts. Watch the tutorial video Key References Bi, Q., Goodman, K. E., Kaminsky, J., &amp; Lessler, J. (2019). What is machine learning? A primer for the epidemiologist. American journal of epidemiology, 188(12), 2222-2239. Liu, Y., Chen, P. H. C., Krause, J., &amp; Peng, L. (2019). How to read articles that use machine learning: users guides to the medical literature. Jama, 322(18), 1806-1816. Kuhn M., Johnson K. (2013) [chapter 4] Over-Fitting and Model Tuning. In: Applied Predictive Modeling. Springer, New York, NY Additional useful references: James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2021). An introduction to statistical learning (2nd ed.). New York: springer. Vittinghoff, E., Glidden, D. V., Shiboski, S. C., &amp; McCulloch, C. E. (2011) [chapter 10] Predictor Selection. In: Regression methods in biostatistics: linear, logistic, survival, and repeated measures models. Springer. Steyerberg, Ewout W. Clinical prediction models (2nd ed.). Springer International Publishing, 2019. Version History Version 1.1 was created for course MEDI 504A 001 Emerging Topics in Experimental Medicine - EMRG TOP EXP MED, delivered on 2021W1. Some of the materials were initially prepared for Continuing Professional Development course, UBC Department of Medicine CPD event, November 3, 2020. Feel free to reach out for any comments, corrections, suggestions. Contributor List Ehsan Karim (SPPH, UBC) License The online version of this book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. You may share, adapt the content and may distribute your contributions under the same license (CC BY-NC-SA 4.0), but you have to give appropriate credit, and cannot use material for the commercial purposes. How to cite Karim, ME (2021) Understanding the Basics and Usage of Machine Learning in Medical Literature, URL: ehsanx.github.io/intro2ML/, (v1.1). "],["rhc-data-description.html", "Chapter 1 RHC data description 1.1 Data download 1.2 Creating Analytic data 1.3 Notations 1.4 Basic data exploration 1.5 Predictive vs. causal models", " Chapter 1 RHC data description Connors et al. (1996) published an article in JAMA. The article is about managing or guiding therapy for the critically ill patients in the intensive care unit. They considered a number of health-outcomes such as length of stay (hospital stay; measured continuously) death within certain period (death at any time up to 180 Days; measured as a binary variable) The original article was concerned about the association of right heart catheterization (RHC) use during the first 24 hours of care in the intensive care unit and the health-outcomes mentioned above, but we will use this data as a case study for our prediction modelling. 1.1 Data download Data is freely available from Vanderbilt Biostatistics, variable liste is available here, and the article is freely available from researchgate. # load the dataset ObsData &lt;- read.csv(&quot;https://hbiostat.org/data/repo/rhc.csv&quot;, header = TRUE) saveRDS(ObsData, file = &quot;data/rhc.RDS&quot;) 1.2 Creating Analytic data In this section, we show the process of preparing analytic data, so that the variables generally match with the way they were coded in the original article. Below we show the process of creating the analytic data. 1.2.1 Add column for outcome: length of stay # Length.of.Stay = date of discharge - study admission date # Length.of.Stay = date of death - study admission date # if date of discharge not available ObsData$Length.of.Stay &lt;- ObsData$dschdte - ObsData$sadmdte ObsData$Length.of.Stay[is.na(ObsData$Length.of.Stay)] &lt;- ObsData$dthdte[is.na(ObsData$Length.of.Stay)] - ObsData$sadmdte[is.na(ObsData$Length.of.Stay)] 1.2.2 Recoding column for outcome: death ObsData$death &lt;- ifelse(ObsData$death == &quot;Yes&quot;, 1, 0) 1.2.3 Remove unnecessary outcomes ObsData &lt;- dplyr::select(ObsData, !c(dthdte, lstctdte, dschdte, t3d30, dth30, surv2md1)) 1.2.4 Remove unnecessary and problematic variables ObsData &lt;- dplyr::select(ObsData, !c(sadmdte, ptid, X, adld3p, urin1, cat2)) 1.2.5 Basic data cleanup # convert all categorical variables to factors factors &lt;- c(&quot;cat1&quot;, &quot;ca&quot;, &quot;death&quot;, &quot;cardiohx&quot;, &quot;chfhx&quot;, &quot;dementhx&quot;, &quot;psychhx&quot;, &quot;chrpulhx&quot;, &quot;renalhx&quot;, &quot;liverhx&quot;, &quot;gibledhx&quot;, &quot;malighx&quot;, &quot;immunhx&quot;, &quot;transhx&quot;, &quot;amihx&quot;, &quot;sex&quot;, &quot;dnr1&quot;, &quot;ninsclas&quot;, &quot;resp&quot;, &quot;card&quot;, &quot;neuro&quot;, &quot;gastr&quot;, &quot;renal&quot;, &quot;meta&quot;, &quot;hema&quot;, &quot;seps&quot;, &quot;trauma&quot;, &quot;ortho&quot;, &quot;race&quot;, &quot;income&quot;) ObsData[factors] &lt;- lapply(ObsData[factors], as.factor) # convert RHC.use (RHC vs. No RHC) to a binary variable ObsData$RHC.use &lt;- ifelse(ObsData$swang1 == &quot;RHC&quot;, 1, 0) ObsData &lt;- dplyr::select(ObsData, !swang1) # Categorize the variables to match with the original paper ObsData$age &lt;- cut(ObsData$age, breaks=c(-Inf, 50, 60, 70, 80, Inf), right=FALSE) ObsData$race &lt;- factor(ObsData$race, levels=c(&quot;white&quot;,&quot;black&quot;,&quot;other&quot;)) ObsData$sex &lt;- as.factor(ObsData$sex) ObsData$sex &lt;- relevel(ObsData$sex, ref = &quot;Male&quot;) ObsData$cat1 &lt;- as.factor(ObsData$cat1) levels(ObsData$cat1) &lt;- c(&quot;ARF&quot;,&quot;CHF&quot;,&quot;Other&quot;,&quot;Other&quot;,&quot;Other&quot;, &quot;Other&quot;,&quot;Other&quot;,&quot;MOSF&quot;,&quot;MOSF&quot;) ObsData$ca &lt;- as.factor(ObsData$ca) levels(ObsData$ca) &lt;- c(&quot;Metastatic&quot;,&quot;None&quot;,&quot;Localized (Yes)&quot;) ObsData$ca &lt;- factor(ObsData$ca, levels=c(&quot;None&quot;, &quot;Localized (Yes)&quot;, &quot;Metastatic&quot;)) 1.2.6 Rename variables names(ObsData) &lt;- c(&quot;Disease.category&quot;, &quot;Cancer&quot;, &quot;Death&quot;, &quot;Cardiovascular&quot;, &quot;Congestive.HF&quot;, &quot;Dementia&quot;, &quot;Psychiatric&quot;, &quot;Pulmonary&quot;, &quot;Renal&quot;, &quot;Hepatic&quot;, &quot;GI.Bleed&quot;, &quot;Tumor&quot;, &quot;Immunosupperssion&quot;, &quot;Transfer.hx&quot;, &quot;MI&quot;, &quot;age&quot;, &quot;sex&quot;, &quot;edu&quot;, &quot;DASIndex&quot;, &quot;APACHE.score&quot;, &quot;Glasgow.Coma.Score&quot;, &quot;blood.pressure&quot;, &quot;WBC&quot;, &quot;Heart.rate&quot;, &quot;Respiratory.rate&quot;, &quot;Temperature&quot;, &quot;PaO2vs.FIO2&quot;, &quot;Albumin&quot;, &quot;Hematocrit&quot;, &quot;Bilirubin&quot;, &quot;Creatinine&quot;, &quot;Sodium&quot;, &quot;Potassium&quot;, &quot;PaCo2&quot;, &quot;PH&quot;, &quot;Weight&quot;, &quot;DNR.status&quot;, &quot;Medical.insurance&quot;, &quot;Respiratory.Diag&quot;, &quot;Cardiovascular.Diag&quot;, &quot;Neurological.Diag&quot;, &quot;Gastrointestinal.Diag&quot;, &quot;Renal.Diag&quot;, &quot;Metabolic.Diag&quot;, &quot;Hematologic.Diag&quot;, &quot;Sepsis.Diag&quot;, &quot;Trauma.Diag&quot;, &quot;Orthopedic.Diag&quot;, &quot;race&quot;, &quot;income&quot;, &quot;Length.of.Stay&quot;, &quot;RHC.use&quot;) saveRDS(ObsData, file = &quot;data/rhcAnalytic.RDS&quot;) 1.3 Notations Notations Example in RHC study \\(Y_1\\): Observed outcome length of stay \\(Y_2\\): Observed outcome death within 3 months \\(L\\): Covariates See below 1.4 Basic data exploration 1.4.1 Dimension and summary dim(ObsData) ## [1] 5735 52 #str(ObsData) 1.4.2 More comprehensive summary require(skimr) ## Loading required package: skimr ## Warning: package &#39;skimr&#39; was built under R version 4.1.1 skim(ObsData) Table 1.1: Data summary Name ObsData Number of rows 5735 Number of columns 52 _______________________ Column type frequency: factor 31 numeric 21 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts Disease.category 0 1 FALSE 4 ARF: 2490, MOS: 1626, Oth: 1163, CHF: 456 Cancer 0 1 FALSE 3 Non: 4379, Loc: 972, Met: 384 Death 0 1 FALSE 2 1: 3722, 0: 2013 Cardiovascular 0 1 FALSE 2 0: 4722, 1: 1013 Congestive.HF 0 1 FALSE 2 0: 4714, 1: 1021 Dementia 0 1 FALSE 2 0: 5171, 1: 564 Psychiatric 0 1 FALSE 2 0: 5349, 1: 386 Pulmonary 0 1 FALSE 2 0: 4646, 1: 1089 Renal 0 1 FALSE 2 0: 5480, 1: 255 Hepatic 0 1 FALSE 2 0: 5334, 1: 401 GI.Bleed 0 1 FALSE 2 0: 5550, 1: 185 Tumor 0 1 FALSE 2 0: 4419, 1: 1316 Immunosupperssion 0 1 FALSE 2 0: 4192, 1: 1543 Transfer.hx 0 1 FALSE 2 0: 5073, 1: 662 MI 0 1 FALSE 2 0: 5535, 1: 200 age 0 1 FALSE 5 [-I: 1424, [60: 1389, [70: 1338, [50: 917 sex 0 1 FALSE 2 Mal: 3192, Fem: 2543 DNR.status 0 1 FALSE 2 No: 5081, Yes: 654 Medical.insurance 0 1 FALSE 6 Pri: 1698, Med: 1458, Pri: 1236, Med: 647 Respiratory.Diag 0 1 FALSE 2 No: 3622, Yes: 2113 Cardiovascular.Diag 0 1 FALSE 2 No: 3804, Yes: 1931 Neurological.Diag 0 1 FALSE 2 No: 5042, Yes: 693 Gastrointestinal.Diag 0 1 FALSE 2 No: 4793, Yes: 942 Renal.Diag 0 1 FALSE 2 No: 5440, Yes: 295 Metabolic.Diag 0 1 FALSE 2 No: 5470, Yes: 265 Hematologic.Diag 0 1 FALSE 2 No: 5381, Yes: 354 Sepsis.Diag 0 1 FALSE 2 No: 4704, Yes: 1031 Trauma.Diag 0 1 FALSE 2 No: 5683, Yes: 52 Orthopedic.Diag 0 1 FALSE 2 No: 5728, Yes: 7 race 0 1 FALSE 3 whi: 4460, bla: 920, oth: 355 income 0 1 FALSE 4 Und: 3226, $11: 1165, $25: 893, &gt; $: 451 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist edu 0 1 11.68 3.15 0.00 10.00 12.00 13.00 30.00  DASIndex 0 1 20.50 5.32 11.00 16.06 19.75 23.43 33.00  APACHE.score 0 1 54.67 19.96 3.00 41.00 54.00 67.00 147.00  Glasgow.Coma.Score 0 1 21.00 30.27 0.00 0.00 0.00 41.00 100.00  blood.pressure 0 1 78.52 38.05 0.00 50.00 63.00 115.00 259.00  WBC 0 1 15.65 11.87 0.00 8.40 14.10 20.05 192.00  Heart.rate 0 1 115.18 41.24 0.00 97.00 124.00 141.00 250.00  Respiratory.rate 0 1 28.09 14.08 0.00 14.00 30.00 38.00 100.00  Temperature 0 1 37.62 1.77 27.00 36.09 38.09 39.00 43.00  PaO2vs.FIO2 0 1 222.27 114.95 11.60 133.31 202.50 316.62 937.50  Albumin 0 1 3.09 0.78 0.30 2.60 3.50 3.50 29.00  Hematocrit 0 1 31.87 8.36 2.00 26.10 30.00 36.30 66.19  Bilirubin 0 1 2.27 4.80 0.10 0.80 1.01 1.40 58.20  Creatinine 0 1 2.13 2.05 0.10 1.00 1.50 2.40 25.10  Sodium 0 1 136.77 7.66 101.00 132.00 136.00 142.00 178.00  Potassium 0 1 4.07 1.03 1.10 3.40 3.80 4.60 11.90  PaCo2 0 1 38.75 13.18 1.00 31.00 37.00 42.00 156.00  PH 0 1 7.39 0.11 6.58 7.34 7.40 7.46 7.77  Weight 0 1 67.83 29.06 0.00 56.30 70.00 83.70 244.00  Length.of.Stay 0 1 21.56 25.87 2.00 7.00 14.00 25.00 394.00  RHC.use 0 1 0.38 0.49 0.00 0.00 0.00 1.00 1.00  #require(rms) #describe(ObsData) 1.5 Predictive vs. causal models The focus of current document is predictive models (e.g., predicting a health outcome). The original article by Connors et al. (1996) focused on the association of right heart catheterization (RHC) use during the first 24 hours of care in the intensive care unit (exposure of primary interest) and the health-outcomes (such as length of stay). If the readers are interested about the causal models used in that article, they can refer to this tutorial. This data has been used in other articles in the literature within the advanced causal modelling context; for example Keele and Small (2021) and Keele and Small (2018). Readers can further consult this tutorial to understand those methods. References "],["prediction-from-continuous-outcome.html", "Chapter 2 Prediction from continuous outcome 2.1 Read previously saved data 2.2 Prediction for length of stay 2.3 Variables 2.4 Model 2.5 Measuring prediction error 2.6 Overfitting and Optimism", " Chapter 2 Prediction from continuous outcome In this chapter, we will talk about Regression that deals with prediction of continuous outcomes. We will use multiple linear regression to build the first prediction mode. 2.1 Read previously saved data ObsData &lt;- readRDS(file = &quot;data/rhcAnalytic.RDS&quot;) 2.2 Prediction for length of stay In this section, we show the regression fitting when outcome is continuous (length of stay). 2.3 Variables baselinevars &lt;- names(dplyr::select(ObsData, !c(Length.of.Stay,Death))) baselinevars ## [1] &quot;Disease.category&quot; &quot;Cancer&quot; &quot;Cardiovascular&quot; ## [4] &quot;Congestive.HF&quot; &quot;Dementia&quot; &quot;Psychiatric&quot; ## [7] &quot;Pulmonary&quot; &quot;Renal&quot; &quot;Hepatic&quot; ## [10] &quot;GI.Bleed&quot; &quot;Tumor&quot; &quot;Immunosupperssion&quot; ## [13] &quot;Transfer.hx&quot; &quot;MI&quot; &quot;age&quot; ## [16] &quot;sex&quot; &quot;edu&quot; &quot;DASIndex&quot; ## [19] &quot;APACHE.score&quot; &quot;Glasgow.Coma.Score&quot; &quot;blood.pressure&quot; ## [22] &quot;WBC&quot; &quot;Heart.rate&quot; &quot;Respiratory.rate&quot; ## [25] &quot;Temperature&quot; &quot;PaO2vs.FIO2&quot; &quot;Albumin&quot; ## [28] &quot;Hematocrit&quot; &quot;Bilirubin&quot; &quot;Creatinine&quot; ## [31] &quot;Sodium&quot; &quot;Potassium&quot; &quot;PaCo2&quot; ## [34] &quot;PH&quot; &quot;Weight&quot; &quot;DNR.status&quot; ## [37] &quot;Medical.insurance&quot; &quot;Respiratory.Diag&quot; &quot;Cardiovascular.Diag&quot; ## [40] &quot;Neurological.Diag&quot; &quot;Gastrointestinal.Diag&quot; &quot;Renal.Diag&quot; ## [43] &quot;Metabolic.Diag&quot; &quot;Hematologic.Diag&quot; &quot;Sepsis.Diag&quot; ## [46] &quot;Trauma.Diag&quot; &quot;Orthopedic.Diag&quot; &quot;race&quot; ## [49] &quot;income&quot; &quot;RHC.use&quot; 2.4 Model # adjust covariates out.formula1 &lt;- as.formula(paste(&quot;Length.of.Stay~ &quot;, paste(baselinevars, collapse = &quot;+&quot;))) saveRDS(out.formula1, file = &quot;data/form1.RDS&quot;) fit1 &lt;- lm(out.formula1, data = ObsData) require(Publish) adj.fit1 &lt;- publish(fit1, digits=1)$regressionTable out.formula1 ## Length.of.Stay ~ Disease.category + Cancer + Cardiovascular + ## Congestive.HF + Dementia + Psychiatric + Pulmonary + Renal + ## Hepatic + GI.Bleed + Tumor + Immunosupperssion + Transfer.hx + ## MI + age + sex + edu + DASIndex + APACHE.score + Glasgow.Coma.Score + ## blood.pressure + WBC + Heart.rate + Respiratory.rate + Temperature + ## PaO2vs.FIO2 + Albumin + Hematocrit + Bilirubin + Creatinine + ## Sodium + Potassium + PaCo2 + PH + Weight + DNR.status + Medical.insurance + ## Respiratory.Diag + Cardiovascular.Diag + Neurological.Diag + ## Gastrointestinal.Diag + Renal.Diag + Metabolic.Diag + Hematologic.Diag + ## Sepsis.Diag + Trauma.Diag + Orthopedic.Diag + race + income + ## RHC.use adj.fit1 ## Variable Units Coefficient CI.95 p-value ## 1 (Intercept) -76.8 [-139.4;-14.2] &lt;0.1 ## 2 Disease.category ARF Ref ## 3 CHF -5.6 [-9.0;-2.2] &lt;0.1 ## 4 Other -4.4 [-6.5;-2.3] &lt;0.1 ## 5 MOSF 2.9 [1.1;4.7] &lt;0.1 ## 6 Cancer None Ref ## 7 Localized (Yes) -7.8 [-15.7;0.1] &lt;0.1 ## 8 Metastatic -10.6 [-19.1;-2.1] &lt;0.1 ## 9 Cardiovascular 0 Ref ## 10 1 0.7 [-1.3;2.7] 0.5 ## 11 Congestive.HF 0 Ref ## 12 1 -1.8 [-3.9;0.4] 0.1 ## 13 Dementia 0 Ref ## 14 1 -1.3 [-3.6;1.1] 0.3 ## 15 Psychiatric 0 Ref ## 16 1 -0.5 [-3.1;2.2] 0.7 ## 17 Pulmonary 0 Ref ## 18 1 2.1 [0.1;4.0] &lt;0.1 ## 19 Renal 0 Ref ## 20 1 -6.9 [-10.8;-3.1] &lt;0.1 ## 21 Hepatic 0 Ref ## 22 1 -1.5 [-5.1;2.1] 0.4 ## 23 GI.Bleed 0 Ref ## 24 1 -5.1 [-9.7;-0.5] &lt;0.1 ## 25 Tumor 0 Ref ## 26 1 4.6 [-3.4;12.6] 0.3 ## 27 Immunosupperssion 0 Ref ## 28 1 0.1 [-1.4;1.7] 0.9 ## 29 Transfer.hx 0 Ref ## 30 1 1.2 [-0.9;3.2] 0.3 ## 31 MI 0 Ref ## 32 1 -1.7 [-5.3;2.0] 0.4 ## 33 age [-Inf,50) Ref ## 34 [50,60) 0.1 [-2.0;2.3] 0.9 ## 35 [60,70) -0.4 [-2.5;1.7] 0.7 ## 36 [70,80) -1.1 [-3.6;1.4] 0.4 ## 37 [80, Inf) -2.8 [-5.7;0.2] &lt;0.1 ## 38 sex Male Ref ## 39 Female 0.8 [-0.6;2.2] 0.2 ## 40 edu 0.0 [-0.2;0.3] 0.7 ## 41 DASIndex -0.1 [-0.2;0.1] 0.4 ## 42 APACHE.score -0.1 [-0.1;-0.0] &lt;0.1 ## 43 Glasgow.Coma.Score 0.0 [-0.0;0.0] 0.2 ## 44 blood.pressure -0.0 [-0.0;0.0] 0.2 ## 45 WBC 0.0 [-0.0;0.1] 0.2 ## 46 Heart.rate 0.0 [0.0;0.0] &lt;0.1 ## 47 Respiratory.rate -0.0 [-0.1;0.1] 1.0 ## 48 Temperature 0.5 [0.1;0.9] &lt;0.1 ## 49 PaO2vs.FIO2 -0.0 [-0.0;-0.0] &lt;0.1 ## 50 Albumin -2.6 [-3.5;-1.6] &lt;0.1 ## 51 Hematocrit -0.2 [-0.3;-0.1] &lt;0.1 ## 52 Bilirubin -0.1 [-0.3;0.1] 0.2 ## 53 Creatinine 0.5 [0.1;1.0] &lt;0.1 ## 54 Sodium 0.1 [0.0;0.2] &lt;0.1 ## 55 Potassium 0.3 [-0.3;1.0] 0.3 ## 56 PaCo2 0.1 [0.0;0.2] &lt;0.1 ## 57 PH 10.1 [2.2;17.9] &lt;0.1 ## 58 Weight 0.0 [-0.0;0.0] 1.0 ## 59 DNR.status No Ref ## 60 Yes -8.0 [-10.1;-5.8] &lt;0.1 ## 61 Medical.insurance Medicaid Ref ## 62 Medicare -0.5 [-3.2;2.2] 0.7 ## 63 Medicare &amp; Medicaid -2.4 [-5.8;0.9] 0.2 ## 64 No insurance -1.8 [-5.2;1.6] 0.3 ## 65 Private -2.1 [-4.6;0.4] 0.1 ## 66 Private &amp; Medicare -2.0 [-4.8;0.8] 0.2 ## 67 Respiratory.Diag No Ref ## 68 Yes 0.3 [-1.4;2.0] 0.7 ## 69 Cardiovascular.Diag No Ref ## 70 Yes 0.4 [-1.4;2.1] 0.7 ## 71 Neurological.Diag No Ref ## 72 Yes 3.5 [1.1;6.0] &lt;0.1 ## 73 Gastrointestinal.Diag No Ref ## 74 Yes 2.6 [0.3;4.8] &lt;0.1 ## 75 Renal.Diag No Ref ## 76 Yes 1.8 [-1.4;5.0] 0.3 ## 77 Metabolic.Diag No Ref ## 78 Yes -1.2 [-4.3;2.0] 0.5 ## 79 Hematologic.Diag No Ref ## 80 Yes -3.9 [-6.8;-0.9] &lt;0.1 ## 81 Sepsis.Diag No Ref ## 82 Yes 0.0 [-2.0;2.0] 1.0 ## 83 Trauma.Diag No Ref ## 84 Yes 1.1 [-5.9;8.2] 0.8 ## 85 Orthopedic.Diag No Ref ## 86 Yes 3.5 [-15.1;22.2] 0.7 ## 87 race white Ref ## 88 black -1.1 [-3.1;0.8] 0.2 ## 89 other 0.2 [-2.5;3.0] 0.9 ## 90 income $11-$25k Ref ## 91 $25-$50k 2.5 [0.2;4.7] &lt;0.1 ## 92 &gt; $50k 0.4 [-2.4;3.3] 0.8 ## 93 Under $11k -0.4 [-2.2;1.4] 0.6 ## 94 RHC.use 2.9 [1.4;4.4] &lt;0.1 2.4.1 Design Matrix Notations n is number of observations p is number of covariates Expands factors to a set of dummy variables. dim(ObsData) ## [1] 5735 52 length(attr(terms(out.formula1), &quot;term.labels&quot;)) ## [1] 50 head(model.matrix(fit1)) ## (Intercept) Disease.categoryCHF Disease.categoryOther Disease.categoryMOSF ## 1 1 0 1 0 ## 2 1 0 0 1 ## 3 1 0 0 1 ## 4 1 0 0 0 ## 5 1 0 0 1 ## 6 1 0 1 0 ## CancerLocalized (Yes) CancerMetastatic Cardiovascular1 Congestive.HF1 ## 1 1 0 0 0 ## 2 0 0 1 1 ## 3 1 0 0 0 ## 4 0 0 0 0 ## 5 0 0 0 0 ## 6 0 0 0 1 ## Dementia1 Psychiatric1 Pulmonary1 Renal1 Hepatic1 GI.Bleed1 Tumor1 ## 1 0 0 1 0 0 0 1 ## 2 0 0 0 0 0 0 0 ## 3 0 0 0 0 0 0 1 ## 4 0 0 0 0 0 0 0 ## 5 0 0 0 0 0 0 0 ## 6 0 0 1 0 0 0 0 ## Immunosupperssion1 Transfer.hx1 MI1 age[50,60) age[60,70) age[70,80) ## 1 0 0 0 0 0 1 ## 2 1 1 0 0 0 1 ## 3 1 0 0 0 0 0 ## 4 1 0 0 0 0 1 ## 5 0 0 0 0 1 0 ## 6 0 0 0 0 0 0 ## age[80, Inf) sexFemale edu DASIndex APACHE.score Glasgow.Coma.Score ## 1 0 0 12.000000 23.50000 46 0 ## 2 0 1 12.000000 14.75195 50 0 ## 3 0 1 14.069916 18.13672 82 0 ## 4 0 1 9.000000 22.92969 48 0 ## 5 0 0 9.945259 21.05078 72 41 ## 6 1 1 8.000000 17.50000 38 0 ## blood.pressure WBC Heart.rate Respiratory.rate Temperature ## 1 41 22.09765620 124 10 38.69531 ## 2 63 28.89843750 137 38 38.89844 ## 3 57 0.04999542 130 40 36.39844 ## 4 55 23.29687500 58 26 35.79688 ## 5 65 29.69921880 125 27 34.79688 ## 6 115 18.00000000 134 36 39.19531 ## PaO2vs.FIO2 Albumin Hematocrit Bilirubin Creatinine Sodium Potassium PaCo2 ## 1 68.0000 3.500000 58.00000 1.0097656 1.1999512 145 4.000000 40 ## 2 218.3125 2.599609 32.50000 0.6999512 0.5999756 137 3.299805 34 ## 3 275.5000 3.500000 21.09766 1.0097656 2.5996094 146 2.899902 16 ## 4 156.6562 3.500000 26.29688 0.3999634 1.6999512 117 5.799805 30 ## 5 478.0000 3.500000 24.00000 1.0097656 3.5996094 126 5.799805 17 ## 6 184.1875 3.099609 30.50000 1.0097656 1.3999023 138 5.399414 68 ## PH Weight DNR.statusYes Medical.insuranceMedicare ## 1 7.359375 64.69995 0 1 ## 2 7.329102 45.69998 0 0 ## 3 7.359375 0.00000 0 0 ## 4 7.459961 54.59998 0 0 ## 5 7.229492 78.39996 1 1 ## 6 7.299805 54.89999 0 1 ## Medical.insuranceMedicare &amp; Medicaid Medical.insuranceNo insurance ## 1 0 0 ## 2 0 0 ## 3 0 0 ## 4 0 0 ## 5 0 0 ## 6 0 0 ## Medical.insurancePrivate Medical.insurancePrivate &amp; Medicare ## 1 0 0 ## 2 0 1 ## 3 1 0 ## 4 0 1 ## 5 0 0 ## 6 0 0 ## Respiratory.DiagYes Cardiovascular.DiagYes Neurological.DiagYes ## 1 1 1 0 ## 2 0 0 0 ## 3 0 1 0 ## 4 1 0 0 ## 5 0 1 0 ## 6 1 0 0 ## Gastrointestinal.DiagYes Renal.DiagYes Metabolic.DiagYes Hematologic.DiagYes ## 1 0 0 0 0 ## 2 0 0 0 0 ## 3 0 0 0 0 ## 4 0 0 0 0 ## 5 0 0 0 0 ## 6 0 0 0 0 ## Sepsis.DiagYes Trauma.DiagYes Orthopedic.DiagYes raceblack raceother ## 1 0 0 0 0 0 ## 2 1 0 0 0 0 ## 3 0 0 0 0 0 ## 4 0 0 0 0 0 ## 5 0 0 0 0 0 ## 6 0 0 0 0 0 ## income$25-$50k income&gt; $50k incomeUnder $11k RHC.use ## 1 0 0 1 0 ## 2 0 0 1 1 ## 3 1 0 0 1 ## 4 0 0 0 0 ## 5 0 0 1 1 ## 6 0 0 1 0 dim(model.matrix(fit1)) ## [1] 5735 64 p &lt;- dim(model.matrix(fit1))[2] # intercept + slopes p ## [1] 64 2.4.2 Obtain prediction obs.y &lt;- ObsData$Length.of.Stay summary(obs.y) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.00 7.00 14.00 21.56 25.00 394.00 # Predict the above fit on ObsData data pred.y1 &lt;- predict(fit1, ObsData) summary(pred.y1) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -32.76 16.62 21.96 21.56 26.73 42.67 n &lt;- length(pred.y1) n ## [1] 5735 plot(obs.y,pred.y1) lines(lowess(obs.y,pred.y1), col = &quot;red&quot;) 2.5 Measuring prediction error Prediction error measures how well the model can predict the outcome for new data that were not used in developing the prediction model. Bias reduced for models with more variables Unimportant variables lead to noise / variability Bias variance trade-off / need penalization 2.5.1 R2 ref # Find SSE SSE &lt;- sum( (obs.y - pred.y1)^2 ) SSE ## [1] 3536398 # Find SST mean.obs.y &lt;- mean(obs.y) SST &lt;- sum( (obs.y - mean.obs.y)^2 ) SST ## [1] 3836690 # Find R2 R.2 &lt;- 1- SSE/SST R.2 ## [1] 0.07826832 require(caret) R2(pred.y1, obs.y) ## [1] 0.07826832 2.5.2 RMSE ref # Find RMSE Rmse &lt;- sqrt(SSE/(n-p)) Rmse ## [1] 24.97185 RMSE(pred.y1, obs.y) ## [1] 24.83212 2.5.3 Adj R2 ref # Find adj R2 adjR2 &lt;- 1-(1-R.2)*((n-1)/(n-p)) adjR2 ## [1] 0.06802866 2.6 Overfitting and Optimism Model usually performs very well in the empirical data where the model was fitted in the same data (optimistic) Model performs poorly in the new data (generalization is not as good) 2.6.1 Causes Model determined by data at hand without expert opinion Too many model parameters (\\(age\\), \\(age^2\\), \\(age^3\\)) / predictors Too small dataset (training) / data too noisy 2.6.2 Consequences Overestimation of effects of predictors Reduction in model performance in new observations 2.6.3 Proposed solutions We generally use procedures such as Internal validation sample splitting cross-validation bootstrap External validation Temporal Geographical Different data source to calculate same variable Different disease "],["data-spliting.html", "Chapter 3 Data spliting 3.1 Read previously saved data 3.2 Split the data 3.3 Train the model 3.4 Extract performance measures", " Chapter 3 Data spliting 3.1 Read previously saved data ObsData &lt;- readRDS(file = &quot;data/rhcAnalytic.RDS&quot;) ref ref # Using a seed to randomize in a reproducible way set.seed(123) require(caret) split&lt;-createDataPartition(y = ObsData$Length.of.Stay, p = 0.7, list = FALSE) str(split) ## int [1:4017, 1] 1 2 3 4 5 6 7 8 9 10 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr &quot;Resample1&quot; dim(split) ## [1] 4017 1 dim(ObsData)*.7 # approximate train data ## [1] 4014.5 36.4 dim(ObsData)*(1-.7) # approximate train data ## [1] 1720.5 15.6 3.2 Split the data # create train data train.data&lt;-ObsData[split,] dim(train.data) ## [1] 4017 52 # create test data test.data&lt;-ObsData[-split,] dim(test.data) ## [1] 1718 52 3.3 Train the model out.formula1 &lt;- readRDS(file = &quot;data/form1.RDS&quot;) out.formula1 ## Length.of.Stay ~ Disease.category + Cancer + Cardiovascular + ## Congestive.HF + Dementia + Psychiatric + Pulmonary + Renal + ## Hepatic + GI.Bleed + Tumor + Immunosupperssion + Transfer.hx + ## MI + age + sex + edu + DASIndex + APACHE.score + Glasgow.Coma.Score + ## blood.pressure + WBC + Heart.rate + Respiratory.rate + Temperature + ## PaO2vs.FIO2 + Albumin + Hematocrit + Bilirubin + Creatinine + ## Sodium + Potassium + PaCo2 + PH + Weight + DNR.status + Medical.insurance + ## Respiratory.Diag + Cardiovascular.Diag + Neurological.Diag + ## Gastrointestinal.Diag + Renal.Diag + Metabolic.Diag + Hematologic.Diag + ## Sepsis.Diag + Trauma.Diag + Orthopedic.Diag + race + income + ## RHC.use fit.train1&lt;-lm(out.formula1, data = train.data) # summary(fit.train1) 3.3.1 Function that gives performance measures perform &lt;- function(new.data, model.fit,model.formula=NULL, y.name = &quot;Y&quot;, digits=3){ # data dimension p &lt;- dim(model.matrix(model.fit))[2] # predicted value pred.y &lt;- predict(model.fit, new.data) # sample size n &lt;- length(pred.y) # outcome new.data.y &lt;- as.numeric(new.data[,y.name]) # R2 R2 &lt;- caret:::R2(pred.y, new.data.y) # adj R2 using alternate formula df.residual &lt;- n-p adjR2 &lt;- 1-(1-R2)*((n-1)/df.residual) # RMSE RMSE &lt;- caret:::RMSE(pred.y, new.data.y) # combine all of the results res &lt;- round(cbind(n,p,R2,adjR2,RMSE),digits) # returning object return(res) } 3.4 Extract performance measures perform(new.data=train.data, y.name = &quot;Length.of.Stay&quot;, model.fit=fit.train1) ## n p R2 adjR2 RMSE ## [1,] 4017 64 0.081 0.067 24.647 perform(new.data=test.data, y.name = &quot;Length.of.Stay&quot;, model.fit=fit.train1) ## n p R2 adjR2 RMSE ## [1,] 1718 64 0.056 0.02 25.488 perform(new.data=ObsData, y.name = &quot;Length.of.Stay&quot;, model.fit=fit.train1) ## n p R2 adjR2 RMSE ## [1,] 5735 64 0.073 0.063 24.902 "],["cross-validation.html", "Chapter 4 Cross-validation 4.1 Read previously saved data 4.2 k-fold cross-vaildation 4.3 Using caret package to automate", " Chapter 4 Cross-validation 4.1 Read previously saved data ObsData &lt;- readRDS(file = &quot;data/rhcAnalytic.RDS&quot;) out.formula1 &lt;- readRDS(file = &quot;data/form1.RDS&quot;) 4.2 k-fold cross-vaildation ref k = 5 dim(ObsData) ## [1] 5735 52 set.seed(567) # create folds (based on outcome) folds &lt;- createFolds(ObsData$Length.of.Stay, k = k, list = TRUE, returnTrain = TRUE) mode(folds) ## [1] &quot;list&quot; dim(ObsData)*4/5 # approximate training data size ## [1] 4588.0 41.6 dim(ObsData)/5 # approximate test data size ## [1] 1147.0 10.4 length(folds[[1]]) ## [1] 4588 length(folds[[5]]) ## [1] 4587 str(folds[[1]]) ## int [1:4588] 1 2 4 6 7 8 9 10 11 13 ... str(folds[[5]]) ## int [1:4587] 1 3 5 6 7 8 10 11 12 13 ... 4.2.1 Calculation for Fold 1 fold.index &lt;- 1 fold1.train.ids &lt;- folds[[fold.index]] head(fold1.train.ids) ## [1] 1 2 4 6 7 8 fold1.train &lt;- ObsData[fold1.train.ids,] fold1.test &lt;- ObsData[-fold1.train.ids,] out.formula1 ## Length.of.Stay ~ Disease.category + Cancer + Cardiovascular + ## Congestive.HF + Dementia + Psychiatric + Pulmonary + Renal + ## Hepatic + GI.Bleed + Tumor + Immunosupperssion + Transfer.hx + ## MI + age + sex + edu + DASIndex + APACHE.score + Glasgow.Coma.Score + ## blood.pressure + WBC + Heart.rate + Respiratory.rate + Temperature + ## PaO2vs.FIO2 + Albumin + Hematocrit + Bilirubin + Creatinine + ## Sodium + Potassium + PaCo2 + PH + Weight + DNR.status + Medical.insurance + ## Respiratory.Diag + Cardiovascular.Diag + Neurological.Diag + ## Gastrointestinal.Diag + Renal.Diag + Metabolic.Diag + Hematologic.Diag + ## Sepsis.Diag + Trauma.Diag + Orthopedic.Diag + race + income + ## RHC.use model.fit &lt;- lm(out.formula1, data = fold1.train) predictions &lt;- predict(model.fit, newdata = fold1.test) perform(new.data=fold1.test, y.name = &quot;Length.of.Stay&quot;, model.fit=model.fit) ## n p R2 adjR2 RMSE ## [1,] 1147 64 0.051 -0.004 24.86 4.2.2 Calculation for Fold 2 fold.index &lt;- 2 fold1.train.ids &lt;- folds[[fold.index]] head(fold1.train.ids) ## [1] 2 3 4 5 6 7 fold1.train &lt;- ObsData[fold1.train.ids,] fold1.test &lt;- ObsData[-fold1.train.ids,] model.fit &lt;- lm(out.formula1, data = fold1.train) predictions &lt;- predict(model.fit, newdata = fold1.test) perform(new.data=fold1.test, y.name = &quot;Length.of.Stay&quot;, model.fit=model.fit) ## n p R2 adjR2 RMSE ## [1,] 1147 64 0.066 0.011 24.714 4.3 Using caret package to automate ref # Using Caret package set.seed(504) # make a 5-fold CV ctrl&lt;-trainControl(method = &quot;cv&quot;,number = 5) # fit the model with formula = out.formula1 # use training method lm fit.cv&lt;-train(out.formula1, trControl = ctrl, data = ObsData, method = &quot;lm&quot;) fit.cv ## Linear Regression ## ## 5735 samples ## 50 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 4588, 4588, 4587, 4589, 4588 ## Resampling results: ## ## RMSE Rsquared MAE ## 25.05478 0.05980578 15.19515 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE # extract results from each test data summary.res &lt;- fit.cv$resample summary.res ## RMSE Rsquared MAE Resample ## 1 22.45199 0.06463766 14.52080 Fold1 ## 2 27.05869 0.06799916 15.29290 Fold2 ## 3 27.65794 0.06034484 15.51895 Fold3 ## 4 24.55357 0.03892546 15.47073 Fold4 ## 5 23.55174 0.06712180 15.17238 Fold5 mean(fit.cv$resample$Rsquared) ## [1] 0.05980578 sd(fit.cv$resample$Rsquared) ## [1] 0.01204451 mean(fit.cv$resample$RMSE) ## [1] 25.05478 sd(fit.cv$resample$RMSE) ## [1] 2.240366 "],["prediction-from-binary-outcome.html", "Chapter 5 Prediction from binary outcome 5.1 Read previously saved data 5.2 Outcome levels (factor) 5.3 Measuring prediction error 5.4 Variables 5.5 Model 5.6 Measuring prediction error 5.7 Cross-validation using caret 5.8 Variable selection", " Chapter 5 Prediction from binary outcome In this chapter, we will talk about Regression that deals with prediction of binary outcomes. We will use logistic regression to build the first prediction mode. 5.1 Read previously saved data ObsData &lt;- readRDS(file = &quot;data/rhcAnalytic.RDS&quot;) 5.2 Outcome levels (factor) Label Possible values of outcome levels(ObsData$Death)=c(&quot;No&quot;,&quot;Yes&quot;) # this is useful for caret # ref: https://tinyurl.com/caretbin class(ObsData$Death) ## [1] &quot;factor&quot; table(ObsData$Death) ## ## No Yes ## 2013 3722 5.3 Measuring prediction error Brier score Brier score 0 means perfect prediction, and close to zero means better prediction, 1 being the worst prediction. Less accurate forecasts get higher score in Brier score. AUC The area under a ROC curve is called as a c statistics. c being 0.5 means random prediction and 1 indicates perfect prediction 5.3.1 Prediction for death In this section, we show the regression fitting when outcome is binary (death). 5.4 Variables baselinevars &lt;- names(dplyr::select(ObsData, !c(Length.of.Stay,Death))) baselinevars ## [1] &quot;Disease.category&quot; &quot;Cancer&quot; &quot;Cardiovascular&quot; ## [4] &quot;Congestive.HF&quot; &quot;Dementia&quot; &quot;Psychiatric&quot; ## [7] &quot;Pulmonary&quot; &quot;Renal&quot; &quot;Hepatic&quot; ## [10] &quot;GI.Bleed&quot; &quot;Tumor&quot; &quot;Immunosupperssion&quot; ## [13] &quot;Transfer.hx&quot; &quot;MI&quot; &quot;age&quot; ## [16] &quot;sex&quot; &quot;edu&quot; &quot;DASIndex&quot; ## [19] &quot;APACHE.score&quot; &quot;Glasgow.Coma.Score&quot; &quot;blood.pressure&quot; ## [22] &quot;WBC&quot; &quot;Heart.rate&quot; &quot;Respiratory.rate&quot; ## [25] &quot;Temperature&quot; &quot;PaO2vs.FIO2&quot; &quot;Albumin&quot; ## [28] &quot;Hematocrit&quot; &quot;Bilirubin&quot; &quot;Creatinine&quot; ## [31] &quot;Sodium&quot; &quot;Potassium&quot; &quot;PaCo2&quot; ## [34] &quot;PH&quot; &quot;Weight&quot; &quot;DNR.status&quot; ## [37] &quot;Medical.insurance&quot; &quot;Respiratory.Diag&quot; &quot;Cardiovascular.Diag&quot; ## [40] &quot;Neurological.Diag&quot; &quot;Gastrointestinal.Diag&quot; &quot;Renal.Diag&quot; ## [43] &quot;Metabolic.Diag&quot; &quot;Hematologic.Diag&quot; &quot;Sepsis.Diag&quot; ## [46] &quot;Trauma.Diag&quot; &quot;Orthopedic.Diag&quot; &quot;race&quot; ## [49] &quot;income&quot; &quot;RHC.use&quot; 5.5 Model # adjust covariates out.formula2 &lt;- as.formula(paste(&quot;Death~ &quot;, paste(baselinevars, collapse = &quot;+&quot;))) saveRDS(out.formula2, file = &quot;data/form2.RDS&quot;) fit2 &lt;- glm(out.formula2, data = ObsData, family = binomial(link = &quot;logit&quot;)) require(Publish) adj.fit2 &lt;- publish(fit2, digits=1)$regressionTable out.formula2 ## Death ~ Disease.category + Cancer + Cardiovascular + Congestive.HF + ## Dementia + Psychiatric + Pulmonary + Renal + Hepatic + GI.Bleed + ## Tumor + Immunosupperssion + Transfer.hx + MI + age + sex + ## edu + DASIndex + APACHE.score + Glasgow.Coma.Score + blood.pressure + ## WBC + Heart.rate + Respiratory.rate + Temperature + PaO2vs.FIO2 + ## Albumin + Hematocrit + Bilirubin + Creatinine + Sodium + ## Potassium + PaCo2 + PH + Weight + DNR.status + Medical.insurance + ## Respiratory.Diag + Cardiovascular.Diag + Neurological.Diag + ## Gastrointestinal.Diag + Renal.Diag + Metabolic.Diag + Hematologic.Diag + ## Sepsis.Diag + Trauma.Diag + Orthopedic.Diag + race + income + ## RHC.use adj.fit2 ## Variable Units OddsRatio CI.95 p-value ## 1 Disease.category ARF Ref ## 2 CHF 1.0 [0.8;1.4] 0.8 ## 3 Other 1.6 [1.3;2.0] &lt;0.1 ## 4 MOSF 1.0 [0.9;1.2] 0.7 ## 5 Cancer None Ref ## 6 Localized (Yes) 6.6 [2.2;19.4] &lt;0.1 ## 7 Metastatic 26.4 [8.3;83.6] &lt;0.1 ## 8 Cardiovascular 0 Ref ## 9 1 1.3 [1.0;1.5] &lt;0.1 ## 10 Congestive.HF 0 Ref ## 11 1 1.6 [1.3;1.9] &lt;0.1 ## 12 Dementia 0 Ref ## 13 1 1.3 [1.0;1.6] &lt;0.1 ## 14 Psychiatric 0 Ref ## 15 1 0.9 [0.7;1.2] 0.6 ## 16 Pulmonary 0 Ref ## 17 1 1.0 [0.9;1.2] 0.7 ## 18 Renal 0 Ref ## 19 1 1.3 [0.9;1.9] 0.2 ## 20 Hepatic 0 Ref ## 21 1 1.3 [0.9;1.8] 0.1 ## 22 GI.Bleed 0 Ref ## 23 1 1.2 [0.8;1.9] 0.3 ## 24 Tumor 0 Ref ## 25 1 0.3 [0.1;0.9] &lt;0.1 ## 26 Immunosupperssion 0 Ref ## 27 1 1.2 [1.1;1.4] &lt;0.1 ## 28 Transfer.hx 0 Ref ## 29 1 0.8 [0.7;1.0] &lt;0.1 ## 30 MI 0 Ref ## 31 1 0.8 [0.6;1.2] 0.3 ## 32 age [-Inf,50) Ref ## 33 [50,60) 1.4 [1.2;1.8] &lt;0.1 ## 34 [60,70) 2.1 [1.7;2.5] &lt;0.1 ## 35 [70,80) 2.1 [1.6;2.6] &lt;0.1 ## 36 [80, Inf) 2.8 [2.1;3.8] &lt;0.1 ## 37 sex Male Ref ## 38 Female 0.8 [0.7;0.9] &lt;0.1 ## 39 edu 1.0 [1.0;1.0] 0.4 ## 40 DASIndex 1.0 [0.9;1.0] &lt;0.1 ## 41 APACHE.score 1.0 [1.0;1.0] &lt;0.1 ## 42 Glasgow.Coma.Score 1.0 [1.0;1.0] &lt;0.1 ## 43 blood.pressure 1.0 [1.0;1.0] 0.4 ## 44 WBC 1.0 [1.0;1.0] 0.2 ## 45 Heart.rate 1.0 [1.0;1.0] 0.7 ## 46 Respiratory.rate 1.0 [1.0;1.0] 0.8 ## 47 Temperature 0.9 [0.9;1.0] &lt;0.1 ## 48 PaO2vs.FIO2 1.0 [1.0;1.0] 0.3 ## 49 Albumin 1.0 [0.9;1.1] 0.6 ## 50 Hematocrit 1.0 [1.0;1.0] &lt;0.1 ## 51 Bilirubin 1.0 [1.0;1.1] &lt;0.1 ## 52 Creatinine 1.0 [1.0;1.0] 0.9 ## 53 Sodium 1.0 [1.0;1.0] 0.7 ## 54 Potassium 1.0 [0.9;1.1] 0.9 ## 55 PaCo2 1.0 [1.0;1.0] 0.3 ## 56 PH 1.1 [0.5;2.3] 0.9 ## 57 Weight 1.0 [1.0;1.0] &lt;0.1 ## 58 DNR.status No Ref ## 59 Yes 2.6 [2.0;3.3] &lt;0.1 ## 60 Medical.insurance Medicaid Ref ## 61 Medicare 1.6 [1.2;2.0] &lt;0.1 ## 62 Medicare &amp; Medicaid 1.4 [1.0;1.9] &lt;0.1 ## 63 No insurance 1.5 [1.1;2.0] &lt;0.1 ## 64 Private 1.3 [1.1;1.7] &lt;0.1 ## 65 Private &amp; Medicare 1.3 [1.0;1.7] &lt;0.1 ## 66 Respiratory.Diag No Ref ## 67 Yes 1.2 [1.0;1.4] &lt;0.1 ## 68 Cardiovascular.Diag No Ref ## 69 Yes 1.2 [1.0;1.4] &lt;0.1 ## 70 Neurological.Diag No Ref ## 71 Yes 1.5 [1.2;1.9] &lt;0.1 ## 72 Gastrointestinal.Diag No Ref ## 73 Yes 1.3 [1.1;1.6] &lt;0.1 ## 74 Renal.Diag No Ref ## 75 Yes 0.8 [0.6;1.1] 0.2 ## 76 Metabolic.Diag No Ref ## 77 Yes 1.0 [0.8;1.4] 0.8 ## 78 Hematologic.Diag No Ref ## 79 Yes 2.7 [2.0;3.8] &lt;0.1 ## 80 Sepsis.Diag No Ref ## 81 Yes 1.1 [0.9;1.4] 0.2 ## 82 Trauma.Diag No Ref ## 83 Yes 0.8 [0.4;1.4] 0.4 ## 84 Orthopedic.Diag No Ref ## 85 Yes 1.4 [0.2;8.1] 0.7 ## 86 race white Ref ## 87 black 1.0 [0.8;1.2] 0.9 ## 88 other 1.1 [0.8;1.4] 0.7 ## 89 income $11-$25k Ref ## 90 $25-$50k 0.8 [0.7;1.0] &lt;0.1 ## 91 &gt; $50k 0.8 [0.6;1.1] 0.2 ## 92 Under $11k 1.2 [1.0;1.4] &lt;0.1 ## 93 RHC.use 1.4 [1.2;1.6] &lt;0.1 5.6 Measuring prediction error 5.6.1 AUC require(pROC) ## Loading required package: pROC ## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation. ## ## Attaching package: &#39;pROC&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## cov, smooth, var obs.y2&lt;-ObsData$Death pred.y2 &lt;- predict(fit2, type = &quot;response&quot;) rocobj &lt;- roc(obs.y2, pred.y2) ## Setting levels: control = No, case = Yes ## Setting direction: controls &lt; cases rocobj ## ## Call: ## roc.default(response = obs.y2, predictor = pred.y2) ## ## Data: pred.y2 in 2013 controls (obs.y2 No) &lt; 3722 cases (obs.y2 Yes). ## Area under the curve: 0.7682 plot(rocobj) auc(rocobj) ## Area under the curve: 0.7682 5.6.2 Brier Score require(DescTools) ## Loading required package: DescTools ## Warning: package &#39;DescTools&#39; was built under R version 4.1.1 ## ## Attaching package: &#39;DescTools&#39; ## The following objects are masked from &#39;package:caret&#39;: ## ## MAE, RMSE BrierScore(fit2) ## [1] 0.1812502 5.7 Cross-validation using caret 5.7.1 Basic setup # Using Caret package set.seed(504) # make a 5-fold CV require(caret) ctrl&lt;-trainControl(method = &quot;cv&quot;, number = 5, classProbs = TRUE, summaryFunction = twoClassSummary) # fit the model with formula = out.formula2 # use training method glm (have to specify family) fit.cv.bin&lt;-train(out.formula2, trControl = ctrl, data = ObsData, method = &quot;glm&quot;, family = binomial(), metric=&quot;ROC&quot;) fit.cv.bin ## Generalized Linear Model ## ## 5735 samples ## 50 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 ## Resampling results: ## ## ROC Sens Spec ## 0.7545115 0.4659618 0.8535653 5.7.2 Extract results from each test data summary.res &lt;- fit.cv.bin$resample summary.res ## ROC Sens Spec Resample ## 1 0.7444835 0.4739454 0.8630872 Fold1 ## 2 0.7544836 0.4502488 0.8561828 Fold2 ## 3 0.7786734 0.4739454 0.8738255 Fold3 ## 4 0.7350679 0.4626866 0.8373656 Fold4 ## 5 0.7598488 0.4689826 0.8373656 Fold5 mean(fit.cv.bin$resample$ROC) ## [1] 0.7545115 sd(fit.cv.bin$resample$ROC) ## [1] 0.01651437 5.7.3 More options ctrl&lt;-trainControl(method = &quot;cv&quot;, number = 5, classProbs = TRUE, summaryFunction = twoClassSummary) fit.cv.bin&lt;-train(out.formula2, trControl = ctrl, data = ObsData, method = &quot;glm&quot;, family = binomial(), metric=&quot;ROC&quot;, preProc = c(&quot;center&quot;, &quot;scale&quot;)) fit.cv.bin ## Generalized Linear Model ## ## 5735 samples ## 50 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## Pre-processing: centered (63), scaled (63) ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 4588, 4589, 4587, 4588, 4588 ## Resampling results: ## ## ROC Sens Spec ## 0.7548047 0.4629717 0.8530367 5.8 Variable selection We can also use stepwise regression that uses AIC as a criterion. set.seed(504) ctrl&lt;-trainControl(method = &quot;cv&quot;, number = 5, classProbs = TRUE, summaryFunction = twoClassSummary) fit.cv.bin.aic&lt;-train(out.formula2, trControl = ctrl, data = ObsData, method = &quot;glmStepAIC&quot;, direction =&quot;backward&quot;, family = binomial(), metric=&quot;ROC&quot;) fit.cv.bin.aic ## Generalized Linear Model with Stepwise Feature Selection ## ## 5735 samples ## 50 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 ## Resampling results: ## ## ROC Sens Spec ## 0.7540424 0.464468 0.8562535 summary(fit.cv.bin.aic) ## ## Call: ## NULL ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.8626 -0.9960 0.5052 0.8638 1.9578 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.0783624 0.7822168 1.379 0.168019 ## Disease.categoryOther 0.4495099 0.0919860 4.887 1.03e-06 ## `CancerLocalized (Yes)` 1.8942512 0.5501880 3.443 0.000575 ## CancerMetastatic 3.2703316 0.5858715 5.582 2.38e-08 ## Cardiovascular1 0.2386749 0.0939617 2.540 0.011081 ## Congestive.HF1 0.4539010 0.0971624 4.672 2.99e-06 ## Dementia1 0.2380213 0.1162903 2.047 0.040679 ## Hepatic1 0.3593093 0.1541762 2.331 0.019779 ## Tumor1 -1.2455123 0.5542624 -2.247 0.024630 ## Immunosupperssion1 0.2174294 0.0730803 2.975 0.002928 ## Transfer.hx1 -0.1849029 0.0945679 -1.955 0.050555 ## `age[50,60)` 0.3621248 0.0984288 3.679 0.000234 ## `age[60,70)` 0.6941924 0.0968434 7.168 7.60e-13 ## `age[70,80)` 0.6804939 0.1126637 6.040 1.54e-09 ## `age[80, Inf)` 0.9833851 0.1410563 6.972 3.13e-12 ## sexFemale -0.2805950 0.0653527 -4.294 1.76e-05 ## DASIndex -0.0429272 0.0062191 -6.902 5.11e-12 ## APACHE.score 0.0174907 0.0020017 8.738 &lt; 2e-16 ## Glasgow.Coma.Score 0.0093657 0.0012563 7.455 9.00e-14 ## WBC 0.0044518 0.0030090 1.479 0.139009 ## Temperature -0.0524703 0.0192757 -2.722 0.006487 ## PaO2vs.FIO2 0.0004741 0.0003054 1.552 0.120548 ## Hematocrit -0.0154796 0.0041593 -3.722 0.000198 ## Bilirubin 0.0313087 0.0094004 3.331 0.000867 ## Weight -0.0031548 0.0011213 -2.813 0.004902 ## DNR.statusYes 0.9347360 0.1326924 7.044 1.86e-12 ## Medical.insuranceMedicare 0.4764895 0.1257582 3.789 0.000151 ## `Medical.insuranceMedicare &amp; Medicaid` 0.3364916 0.1584757 2.123 0.033729 ## `Medical.insuranceNo insurance` 0.3711345 0.1568820 2.366 0.017996 ## Medical.insurancePrivate 0.2632637 0.1139805 2.310 0.020903 ## `Medical.insurancePrivate &amp; Medicare` 0.2819715 0.1313101 2.147 0.031764 ## Respiratory.DiagYes 0.1393974 0.0769026 1.813 0.069886 ## Cardiovascular.DiagYes 0.1804967 0.0836679 2.157 0.030982 ## Neurological.DiagYes 0.4320266 0.1189357 3.632 0.000281 ## Gastrointestinal.DiagYes 0.2819563 0.1092206 2.582 0.009836 ## Hematologic.DiagYes 0.9734424 0.1651363 5.895 3.75e-09 ## Sepsis.DiagYes 0.1539651 0.0943235 1.632 0.102614 ## `incomeUnder $11k` 0.2151437 0.0689392 3.121 0.001804 ## RHC.use 0.3552053 0.0713632 4.977 6.44e-07 ## ## (Intercept) ## Disease.categoryOther *** ## `CancerLocalized (Yes)` *** ## CancerMetastatic *** ## Cardiovascular1 * ## Congestive.HF1 *** ## Dementia1 * ## Hepatic1 * ## Tumor1 * ## Immunosupperssion1 ** ## Transfer.hx1 . ## `age[50,60)` *** ## `age[60,70)` *** ## `age[70,80)` *** ## `age[80, Inf)` *** ## sexFemale *** ## DASIndex *** ## APACHE.score *** ## Glasgow.Coma.Score *** ## WBC ## Temperature ** ## PaO2vs.FIO2 ## Hematocrit *** ## Bilirubin *** ## Weight ** ## DNR.statusYes *** ## Medical.insuranceMedicare *** ## `Medical.insuranceMedicare &amp; Medicaid` * ## `Medical.insuranceNo insurance` * ## Medical.insurancePrivate * ## `Medical.insurancePrivate &amp; Medicare` * ## Respiratory.DiagYes . ## Cardiovascular.DiagYes * ## Neurological.DiagYes *** ## Gastrointestinal.DiagYes ** ## Hematologic.DiagYes *** ## Sepsis.DiagYes ## `incomeUnder $11k` ** ## RHC.use *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 7433.3 on 5734 degrees of freedom ## Residual deviance: 6198.0 on 5696 degrees of freedom ## AIC: 6276 ## ## Number of Fisher Scoring iterations: 5 "],["supervised-learning.html", "Chapter 6 Supervised Learning 6.1 Read previously saved data 6.2 Continuous outcome 6.3 Binary outcome 6.4 Ensemble methods (Type I) 6.5 Ensemble methods (Type II)", " Chapter 6 Supervised Learning In this chapter, we will move beyond statistical regression, and introduce some of the popular machine learning methods. 6.1 Read previously saved data ObsData &lt;- readRDS(file = &quot;data/rhcAnalytic.RDS&quot;) levels(ObsData$Death)=c(&quot;No&quot;,&quot;Yes&quot;) out.formula1 &lt;- readRDS(file = &quot;data/form1.RDS&quot;) out.formula2 &lt;- readRDS(file = &quot;data/form2.RDS&quot;) 6.2 Continuous outcome 6.2.1 Cross-validation LASSO ctrl &lt;- trainControl(method = &quot;cv&quot;, number = 5) fit.cv.con &lt;- train(out.formula1, trControl = ctrl, data = ObsData, method = &quot;glmnet&quot;, lambda= 0, tuneGrid = expand.grid(alpha = 1, lambda = 0), verbose = FALSE) fit.cv.con ## glmnet ## ## 5735 samples ## 50 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 4588, 4587, 4587, 4589, 4589 ## Resampling results: ## ## RMSE Rsquared MAE ## 25.14179 0.05746966 15.20167 ## ## Tuning parameter &#39;alpha&#39; was held constant at a value of 1 ## Tuning ## parameter &#39;lambda&#39; was held constant at a value of 0 6.2.2 Cross-validation Ridge ctrl &lt;- trainControl(method = &quot;cv&quot;, number = 5) fit.cv.con &lt;-train(out.formula1, trControl = ctrl, data = ObsData, method = &quot;glmnet&quot;, lambda= 0, tuneGrid = expand.grid(alpha = 0, lambda = 0), verbose = FALSE) fit.cv.con ## glmnet ## ## 5735 samples ## 50 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 4588, 4589, 4587, 4588, 4588 ## Resampling results: ## ## RMSE Rsquared MAE ## 25.0993 0.05746023 15.22744 ## ## Tuning parameter &#39;alpha&#39; was held constant at a value of 0 ## Tuning ## parameter &#39;lambda&#39; was held constant at a value of 0 6.3 Binary outcome 6.3.1 Cross-validation LASSO ctrl&lt;-trainControl(method = &quot;cv&quot;, number = 5, classProbs = TRUE, summaryFunction = twoClassSummary) fit.cv.bin&lt;-train(out.formula2, trControl = ctrl, data = ObsData, method = &quot;glmnet&quot;, lambda= 0, tuneGrid = expand.grid(alpha = 1, lambda = 0), verbose = FALSE, metric=&quot;ROC&quot;) fit.cv.bin ## glmnet ## ## 5735 samples ## 50 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 4588, 4588, 4589, 4588, 4587 ## Resampling results: ## ## ROC Sens Spec ## 0.7546214 0.4634742 0.8535823 ## ## Tuning parameter &#39;alpha&#39; was held constant at a value of 1 ## Tuning ## parameter &#39;lambda&#39; was held constant at a value of 0 Not okay to select variables from a shrinkage model, and then use them in a regular regression 6.3.2 Cross-validation Ridge ctrl&lt;-trainControl(method = &quot;cv&quot;, number = 5, classProbs = TRUE, summaryFunction = twoClassSummary) fit.cv.bin&lt;-train(out.formula2, trControl = ctrl, data = ObsData, method = &quot;glmnet&quot;, lambda= 0, tuneGrid = expand.grid(alpha = 0, lambda = 0), verbose = FALSE, metric=&quot;ROC&quot;) fit.cv.bin ## glmnet ## ## 5735 samples ## 50 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 4588, 4589, 4588, 4587, 4588 ## Resampling results: ## ## ROC Sens Spec ## 0.7528764 0.4610002 0.8524987 ## ## Tuning parameter &#39;alpha&#39; was held constant at a value of 0 ## Tuning ## parameter &#39;lambda&#39; was held constant at a value of 0 6.3.3 Cross-validation Elastic net Alpha = mixing parameter Lambda = regularization or tuning parameter We can use expand.grid for model tuning ctrl&lt;-trainControl(method = &quot;cv&quot;, number = 5, classProbs = TRUE, summaryFunction = twoClassSummary) fit.cv.bin&lt;-train(out.formula2, trControl = ctrl, data = ObsData, method = &quot;glmnet&quot;, tuneGrid = expand.grid(alpha = seq(0.1,.2,by = 0.05), lambda = seq(0.05,0.3,by = 0.05)), verbose = FALSE, metric=&quot;ROC&quot;) fit.cv.bin ## glmnet ## ## 5735 samples ## 50 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 4587, 4588, 4588, 4588, 4589 ## Resampling results across tuning parameters: ## ## alpha lambda ROC Sens Spec ## 0.10 0.05 0.7530058 0.3695986568 0.8965624 ## 0.10 0.10 0.7493765 0.2742170043 0.9336408 ## 0.10 0.15 0.7445050 0.1748601904 0.9677600 ## 0.10 0.20 0.7388230 0.0894189104 0.9871036 ## 0.10 0.25 0.7322015 0.0253323951 0.9959703 ## 0.10 0.30 0.7254256 0.0019863462 0.9997315 ## 0.15 0.05 0.7519700 0.3537017148 0.9035477 ## 0.15 0.10 0.7453497 0.2200659235 0.9529844 ## 0.15 0.15 0.7356439 0.1023307779 0.9849553 ## 0.15 0.20 0.7247385 0.0178820538 0.9959695 ## 0.15 0.25 0.7170856 0.0004962779 0.9997315 ## 0.15 0.30 0.7107273 0.0000000000 1.0000000 ## 0.20 0.05 0.7507079 0.3328284138 0.9140258 ## 0.20 0.10 0.7394339 0.1693974297 0.9674904 ## 0.20 0.15 0.7242395 0.0372492377 0.9919405 ## 0.20 0.20 0.7140885 0.0014900683 0.9997315 ## 0.20 0.25 0.7069905 0.0000000000 1.0000000 ## 0.20 0.30 0.6971477 0.0000000000 1.0000000 ## ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were alpha = 0.1 and lambda = 0.05. plot(fit.cv.bin) 6.3.4 Decision tree Decision tree Referred to as Classification and regression trees or CART Covers Classification (categorical outcome) Regression (continuous outcome) Flexible to incorporate non-linear effects automatically No need to specify higher order terms / interactions Unstable, prone to overfitting, suffers from high variance 6.3.4.1 Simple CART require(rpart) summary(ObsData$DASIndex) # Duke Activity Status Index ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 11.00 16.06 19.75 20.50 23.43 33.00 cart.fit &lt;- rpart(Death~DASIndex, data = ObsData) par(mfrow = c(1,1), xpd = NA) plot(cart.fit) text(cart.fit, use.n = TRUE) print(cart.fit) ## n= 5735 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 5735 2013 Yes (0.3510026 0.6489974) ## 2) DASIndex&gt;=24.92383 1143 514 No (0.5503062 0.4496938) ## 4) DASIndex&gt;=29.14648 561 199 No (0.6452763 0.3547237) * ## 5) DASIndex&lt; 29.14648 582 267 Yes (0.4587629 0.5412371) * ## 3) DASIndex&lt; 24.92383 4592 1384 Yes (0.3013937 0.6986063) * require(rattle) require(rpart.plot) require(RColorBrewer) fancyRpartPlot(cart.fit, caption = NULL) 6.3.4.1.1 AUC require(pROC) obs.y2&lt;-ObsData$Death pred.y2 &lt;- as.numeric(predict(cart.fit, type = &quot;prob&quot;)[, 2]) rocobj &lt;- roc(obs.y2, pred.y2) ## Setting levels: control = No, case = Yes ## Setting direction: controls &lt; cases rocobj ## ## Call: ## roc.default(response = obs.y2, predictor = pred.y2) ## ## Data: pred.y2 in 2013 controls (obs.y2 No) &lt; 3722 cases (obs.y2 Yes). ## Area under the curve: 0.5912 plot(rocobj) auc(rocobj) ## Area under the curve: 0.5912 6.3.4.2 Complex CART More variables out.formula2 ## Death ~ Disease.category + Cancer + Cardiovascular + Congestive.HF + ## Dementia + Psychiatric + Pulmonary + Renal + Hepatic + GI.Bleed + ## Tumor + Immunosupperssion + Transfer.hx + MI + age + sex + ## edu + DASIndex + APACHE.score + Glasgow.Coma.Score + blood.pressure + ## WBC + Heart.rate + Respiratory.rate + Temperature + PaO2vs.FIO2 + ## Albumin + Hematocrit + Bilirubin + Creatinine + Sodium + ## Potassium + PaCo2 + PH + Weight + DNR.status + Medical.insurance + ## Respiratory.Diag + Cardiovascular.Diag + Neurological.Diag + ## Gastrointestinal.Diag + Renal.Diag + Metabolic.Diag + Hematologic.Diag + ## Sepsis.Diag + Trauma.Diag + Orthopedic.Diag + race + income + ## RHC.use require(rpart) cart.fit &lt;- rpart(out.formula2, data = ObsData) 6.3.4.2.1 CART Variable importance cart.fit$variable.importance ## DASIndex Cancer Tumor age ## 123.2102455 33.4559400 32.5418433 24.0804860 ## Medical.insurance WBC edu Cardiovascular.Diag ## 14.5199953 5.6673997 3.7441554 3.6449371 ## Heart.rate Cardiovascular Trauma.Diag PaCo2 ## 3.4059248 3.1669125 0.5953098 0.2420672 ## Potassium Sodium Albumin ## 0.2420672 0.2420672 0.1984366 6.3.4.2.2 AUC require(pROC) obs.y2&lt;-ObsData$Death pred.y2 &lt;- as.numeric(predict(cart.fit, type = &quot;prob&quot;)[, 2]) rocobj &lt;- roc(obs.y2, pred.y2) ## Setting levels: control = No, case = Yes ## Setting direction: controls &lt; cases rocobj ## ## Call: ## roc.default(response = obs.y2, predictor = pred.y2) ## ## Data: pred.y2 in 2013 controls (obs.y2 No) &lt; 3722 cases (obs.y2 Yes). ## Area under the curve: 0.5981 plot(rocobj) auc(rocobj) ## Area under the curve: 0.5981 6.3.4.3 Cross-validation CART set.seed(504) require(caret) ctrl&lt;-trainControl(method = &quot;cv&quot;, number = 5, classProbs = TRUE, summaryFunction = twoClassSummary) # fit the model with formula = out.formula2 fit.cv.bin&lt;-train(out.formula2, trControl = ctrl, data = ObsData, method = &quot;rpart&quot;, metric=&quot;ROC&quot;) fit.cv.bin ## CART ## ## 5735 samples ## 50 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 ## Resampling results across tuning parameters: ## ## cp ROC Sens Spec ## 0.007203179 0.6304911 0.2816488 0.9086574 ## 0.039741679 0.5725283 0.2488649 0.8981807 ## 0.057128664 0.5380544 0.1287804 0.9473284 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.007203179. # extract results from each test data summary.res &lt;- fit.cv.bin$resample summary.res ## ROC Sens Spec Resample ## 1 0.6847220 0.3746898 0.8590604 Fold1 ## 2 0.6729625 0.2985075 0.8924731 Fold2 ## 3 0.6076153 0.2754342 0.9287634 Fold5 ## 4 0.5873154 0.2238806 0.9274194 Fold4 ## 5 0.5998401 0.2357320 0.9355705 Fold3 6.4 Ensemble methods (Type I) Training same model to different samples (of the same data) 6.4.1 Cross-validation bagging Bagging or bootstrap aggregation independent bootstrap samples (sampling with replacement, B times), applies CART on each i (no prunning) Average the resulting predictions Reduces variance as a result of using bootstrap set.seed(504) require(caret) ctrl&lt;-trainControl(method = &quot;cv&quot;, number = 5, classProbs = TRUE, summaryFunction = twoClassSummary) # fit the model with formula = out.formula2 fit.cv.bin&lt;-train(out.formula2, trControl = ctrl, data = ObsData, method = &quot;bag&quot;, bagControl = bagControl(fit = ldaBag$fit, predict = ldaBag$pred, aggregate = ldaBag$aggregate), metric=&quot;ROC&quot;) ## Warning: executing %dopar% sequentially: no parallel backend registered fit.cv.bin ## Bagged Model ## ## 5735 samples ## 50 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 ## Resampling results: ## ## ROC Sens Spec ## 0.7506666 0.4485809 0.8602811 ## ## Tuning parameter &#39;vars&#39; was held constant at a value of 63 bagging improves prediction accuracy over prediction using a single tree Looses interpretability as this is an average of many diagrams now But we can get a summary of the importance of each variable 6.4.1.1 Bagging Variable importance caret::varImp(fit.cv.bin, scale = FALSE) ## ROC curve variable importance ## ## only 20 most important variables shown (out of 50) ## ## Importance ## age 0.6159 ## APACHE.score 0.6140 ## DASIndex 0.5962 ## Cancer 0.5878 ## Creatinine 0.5835 ## Tumor 0.5807 ## blood.pressure 0.5697 ## Glasgow.Coma.Score 0.5656 ## Disease.category 0.5641 ## Temperature 0.5584 ## DNR.status 0.5572 ## Hematocrit 0.5525 ## Weight 0.5424 ## Bilirubin 0.5397 ## income 0.5319 ## Immunosupperssion 0.5278 ## RHC.use 0.5263 ## Dementia 0.5252 ## Congestive.HF 0.5250 ## Hematologic.Diag 0.5250 6.4.2 Cross-validation boosting Boosting sequentially updated/weighted bootstrap based on previous learning set.seed(504) require(caret) ctrl&lt;-trainControl(method = &quot;cv&quot;, number = 5, classProbs = TRUE, summaryFunction = twoClassSummary) # fit the model with formula = out.formula2 fit.cv.bin&lt;-train(out.formula2, trControl = ctrl, data = ObsData, method = &quot;gbm&quot;, verbose = FALSE, metric=&quot;ROC&quot;) fit.cv.bin ## Stochastic Gradient Boosting ## ## 5735 samples ## 50 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 4587, 4589, 4587, 4589, 4588 ## Resampling results across tuning parameters: ## ## interaction.depth n.trees ROC Sens Spec ## 1 50 0.7218938 0.2145970 0.9505647 ## 1 100 0.7410292 0.2980581 0.9234228 ## 1 150 0.7483014 0.3487142 0.9030028 ## 2 50 0.7414513 0.2960631 0.9263816 ## 2 100 0.7534264 0.3869684 0.8917212 ## 2 150 0.7575826 0.4187512 0.8777477 ## 3 50 0.7496078 0.3626125 0.9070358 ## 3 100 0.7579645 0.4078244 0.8764076 ## 3 150 0.7637074 0.4445909 0.8702298 ## ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## ## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10 ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were n.trees = 150, interaction.depth = ## 3, shrinkage = 0.1 and n.minobsinnode = 10. plot(fit.cv.bin) 6.5 Ensemble methods (Type II) Training different models on the same data 6.5.1 Super Learner Large number of candidate learners (CL) with different strengths Parametric (logistic) Non-parametric (CART) Cross-validation: CL applied on training data, prediction made on test data Final prediction uses a weighted version of all predictions Weights = coef of Observed outcome ~ prediction from each CL 6.5.2 Steps Refer to this tutorial for steps and examples! "],["unsupervised-learning.html", "Chapter 7 Unsupervised Learning 7.1 K-means 7.2 Read previously saved data 7.3 Optimal number of clusters 7.4 Discussion", " Chapter 7 Unsupervised Learning Clustering is an unsupervised learning algorithm. These algorithms can classify data into multiple groups. Such classification is based on similarity. Group characteristics include (to the extent that is possible) low inter-class similarity: observation from different clusters would be dissimilar high intra-class similarity: observation from the same cluster would be similar Within-cluster variation will be thus minimized by optimizing within-cluster sum of squares of Euclidean distances (ref) 7.1 K-means K-means is a very popular clustering algorithm, that partitions the data into \\(k\\) groups. Algorithm: Determine a number \\(k\\) (e.g., could be 3) randomly select \\(k\\) subjects in a data. Use these points as staring points (centers or cluster mean) for each cluster. By Euclidean distance measure (from the initial centers), try to determine in which cluster the remaining points belong. compute new mean value for each cluster. based on this new mean, try to determine again in which cluster the data points belong. process continues until the data points do not change cluster membership. 7.2 Read previously saved data ObsData &lt;- readRDS(file = &quot;data/rhcAnalytic.RDS&quot;) 7.2.1 Example 1 datax0 &lt;- ObsData[c(&quot;Heart.rate&quot;, &quot;edu&quot;)] kres0 &lt;- kmeans(datax0, centers = 2, nstart = 10) kres0$centers ## Heart.rate edu ## 1 54.55138 11.44494 ## 2 134.96277 11.75466 plot(datax0, col = kres0$cluster, main = kres0$tot.withinss) 7.2.2 Example 2 datax0 &lt;- ObsData[c(&quot;blood.pressure&quot;, &quot;Heart.rate&quot;, &quot;Respiratory.rate&quot;)] kres0 &lt;- kmeans(datax0, centers = 2, nstart = 10) kres0$centers ## blood.pressure Heart.rate Respiratory.rate ## 1 80.10812 135.08956 29.85267 ## 2 73.71684 54.95789 22.76723 plot(datax0, col = kres0$cluster, main = kres0$tot.withinss) 7.2.3 Example with many variables datax &lt;- ObsData[c(&quot;edu&quot;, &quot;blood.pressure&quot;, &quot;Heart.rate&quot;, &quot;Respiratory.rate&quot; , &quot;Temperature&quot;, &quot;PH&quot;, &quot;Weight&quot;, &quot;Length.of.Stay&quot;)] kres &lt;- kmeans(datax, centers = 3) #kres head(kres$cluster) ## [1] 1 1 1 3 1 2 kres$size ## [1] 2793 1688 1254 kres$centers ## edu blood.pressure Heart.rate Respiratory.rate Temperature PH ## 1 11.85833 54.26924 136.37451 29.76119 37.85078 7.385249 ## 2 11.54214 128.33886 126.12026 29.36611 37.68129 7.401027 ## 3 11.46134 65.47249 53.24242 22.65973 37.01597 7.378482 ## Weight Length.of.Stay ## 1 68.63384 23.42356 ## 2 66.68351 20.68128 ## 3 67.57291 18.58931 aggregate(datax, by = list(cluster = kres$cluster), mean) ## cluster edu blood.pressure Heart.rate Respiratory.rate Temperature ## 1 1 11.85833 54.26924 136.37451 29.76119 37.85078 ## 2 2 11.54214 128.33886 126.12026 29.36611 37.68129 ## 3 3 11.46134 65.47249 53.24242 22.65973 37.01597 ## PH Weight Length.of.Stay ## 1 7.385249 68.63384 23.42356 ## 2 7.401027 66.68351 20.68128 ## 3 7.378482 67.57291 18.58931 aggregate(datax, by = list(cluster = kres$cluster), sd) ## cluster edu blood.pressure Heart.rate Respiratory.rate Temperature ## 1 1 3.162485 11.93763 23.13140 13.67791 1.781692 ## 2 2 3.091605 18.58070 27.68369 14.08169 1.610746 ## 3 3 3.160538 31.89150 23.63993 13.60831 1.832389 ## PH Weight Length.of.Stay ## 1 0.1082140 27.99506 29.01143 ## 2 0.1009567 32.15078 23.37223 ## 3 0.1226041 26.87075 20.82024 7.3 Optimal number of clusters require(factoextra) fviz_nbclust(datax, kmeans, method = &quot;wss&quot;)+ geom_vline(xintercept=3,linetype=3) Here the vertical line is chosen based on elbow method (ref). 7.4 Discussion We need to supply a number, \\(k\\): but we can test different \\(k\\)s to identify optimal value Clustering can be influenced by outliners, so median based clustering is possible mere ordering can influence clustering, hence we should choose different initial means (e.g., nstart should be greater than 1). "],["model-development-considerations.html", "Chapter 8 Model Development Considerations 8.1 Why Machine learning? 8.2 Data pre-processing 8.3 Missing data considerations 8.4 Data hungry methods 8.5 Model tuning 8.6 Time resource requirements", " Chapter 8 Model Development Considerations 8.1 Why Machine learning? Let us assume that our aim is to build a clinical diagnostic tool or risk prediction model. As we have seen earlier, parametric (linear or logistic) regression models are capable of producing prediction models. relies on expert knowledge to select covariates Parsimony vs better prediction There are different Machine Learning (ML) methods that can deal with multicollinearity (e.g., LASSO) model-specification: interaction term, polynomial or other complex functional form (e.g., tree-based) potentially better predictive ability by utilizing many learners (e.g., super learner) parametric methods are efficient, but analyst exactly needs to know the model-specification to get the best results nonparameteric methods are not as efficient, but less restrictive some data adaptive methods are good at transforming the data and find optimal model-specification super learner can combine the predictive ability of different type of learners (parametric vs. not) ability to handle variety of data (e.g., image) ability to handle larger amount of data (e.g., large health admin data; high dimensional data) in identifying variables that are risk factors for the outcome that may not be easily identified by the subject area experts (e.g., may offer new knowledge) Analyst should be absolutely clear why they are using a ML method. Contrary to popular belief, ML method may not always provide better results! REMINDER: A systematic review shows no performance benefit of #machinelearning over logistic regression for clinical prediction models (https://t.co/NA54aJoKHc)-&gt; &quot;Improvements in methodology and reporting are needed&quot;#keepitreal #dontbelievethehype #mlwtitter #statstwitter pic.twitter.com/NYZheoU5ZS&mdash; Gary Collins  (@GSCollins) November 1, 2021 8.2 Data pre-processing Centering will make the mean of variables 0. Scaling the variables will produce a common standard deviation (1). Often these help in bringing numerical stability. Continuous variables may require some transformation to remove skewness or outlier issues of the data. Dichotomising continuous measurements (also known as throwing away precious information - that someone has consented for you to use) is still disappointingly common today. Many papers on this, but a good starting point is -&gt; https://t.co/dEU7etTkaR#statstwitter #epitwitter pic.twitter.com/UaC84LNFFd&mdash; Gary Collins  (@GSCollins) November 10, 2021 8.3 Missing data considerations &quot;Missing data is poorly handled and reported in prediction model studies using #machinelearning: a literature review&quot; --&gt; https://t.co/8qXU5GTy9FLooks like this will feed into and inform @TRIPODStatement-AI --&gt; https://t.co/a1oFJRj4k4#mltwitter #statstwitter #ML4HC pic.twitter.com/BHJ6DIzrGg&mdash; Gary Collins  (@GSCollins) November 18, 2021 The paper reports and recommends (emphasis added): Although many types of machine learning methods offer built-in capabilities for handling missing values, these strategies are rarely used. Instead, most ML-based prediction model studies resort to complete case analysis or mean imputation. The handling and reporting of missing data in prediction model studies should be improved. A general recommendation to avoid bias is to use multiple imputation. It is also possible to consider machine learning methods with built-in capabilities for handling missing data (e.g., decision trees with surrogate splits, use of pattern submodels, or incorporation of autoencoders). This course is not going to discuss much about missing data analyses, but if you are interested, you can see my video lectures (series of 8 are relevant for missing data) from my other course here. For relevant software, see this article. 8.4 Data hungry methods Regression methods require at least 10 events per variable (EPV) EPV: \"  the one in ten rule is a rule of thumb for how many predictor parameters can be estimated from data when doing regression analysis  while keeping the risk of overfitting low. The rule states that one predictive variable can be studied for every ten events.\" Single ML methods may require as 50-200 EPV (depending on which method is chosen). See Chapter 3 of Steyerberg and others (2019) Often simpler models are chosen when sample size is not high. Variable selection via LASSO may be helpful to reduce noisy variables from the model. 8.5 Model tuning Hyperparameters are the parameters that are determined by the analysts, and stays the same during the learning process. Often it is the case that analysts do not know which hyperparameter selection will provide better results, and the analyst may need to do a manual search (based on best guesses) or grid search to find out the best set of hyperparameter. For logistic regression, one example of hyperparameter could be optimization algorithm for fitting the model: NewtonRaphson Fisher Scoring or Iteratively Reweighted Least Squares (IRLS) Hybrid (start with IRLS for initialization purposes, and then NewtonRaphson) Part of the reason these methods may require more time to fit is because of existence of many parameters needing to be optimized to obtain better accuracy of the training model. These are usually known as hyperparameters, and the search process is known as hyperparameter tuning. Usually training and tuning datasets are used to tune the model for finding better hyperparameters. To enhance reproducibility, cross-validation could also be used in the training and tuning process. Validation set and tuning set should be different. If the validation set was used for tuning, then the trained model can still be subject to overfitting. 8.6 Time resource requirements Some methods are inherently based on repeated fitting of the same candidate learner or model in different data (Type - I). Ensemble learning combines predictive ability of many candidate learners on the same data. Need to be careful which candidate learner are selected (Type - II; needs variety). Both of these scenarios may require significantly more time. Also, even some single candidate learners could be time intensive (e.g., deep learning with many layer). It is a good idea to report the computing time. References "],["model-validation-considerations.html", "Chapter 9 Model Validation Considerations 9.1 Reliability of outcome labels 9.2 Robustness of the results", " Chapter 9 Model Validation Considerations 9.1 Reliability of outcome labels Many clinical outcomes come from reliable data sources. Death data is considered hard outcome, usually come from death registry, and are often not subject to much error. However, there are other clinical outcomes that are considered soft outcomes (e.g., subject to mis-classifications, mis-remembering). EDSS scale in multiple sclerosis may be measured differently in different jurisdictions or clinical training practices Number of relapses over past 2 years may be subject to recall bias Unsurprising, quality of outcome data matters in building a good prediction model. 9.2 Robustness of the results Blindly believing or reporting the ML results might not be useful. Researchers should always seek for explanation of analysis results. This is particularly true if the results are somewhat surprising or unexpected. It is always a good idea to check with a person with clinical expert to assess and scrutinize the results. Analysts should always be skeptic of results when there is a large gap between the performances of training (after tuning) and test sets. Analyst should always care about reproducibility and repeatability of the analysis results (requires keeping good log of what parameter, hyperparameter, computational settings settings were used). Indeed internal validity is helpful, but the external validity should also be assessed when possible. Often the models developed under laboratory condition may not perform well under real-world clinical settings. "],["clinical-implication.html", "Chapter 10 Clinical Implication 10.1 ML in clinical settings 10.2 Model updating", " Chapter 10 Clinical Implication 10.1 ML in clinical settings We could have the following uses of ML methods in the clinical settings: prescreen patients to identify high-risk patient pool. warn patients about imminent risk helps manage clinical workload help diagnose a disease better with high accuracy could be based on radiology or pathology images could prevent mis-diagnose by giving a second opinion could indicate suspicious regions, assisting clinicians to focus on the most important considerations Monitoring vulnerable patients monitoring devices (e.g., fall detection) ethical, moral and transparency considerations 10.2 Model updating Updating risk prediction model based on new data could be automated given access to continuously collected data policy could change, requiring the update of the model "],["critical-appraisal.html", "Chapter 11 Critical Appraisal 11.1 Existing guidelines 11.2 Key considerations 11.3 Example 11.4 Exercise", " Chapter 11 Critical Appraisal Was always skeptical about the deluge of ML papers coming out during COVID. Critical appraisal of a Machine Learning paper is an important aspect of modern medicine evidence analysis. pic.twitter.com/8yvnsVbL4L&mdash; vishnu v y (@vishnuvy) June 6, 2021 Given the popularity of ML and AI methods, it is important to be able to critically appraise a paper that reports ML / AI analyses results. 11.1 Existing guidelines Liu et al. (2020) and Rivera et al. (2020) provided the CONSORT-AI and SPIRITAI Extensions. Vinny et al. (2021) reported a number of ways to critically appraise an article that analysed the data using an ML approach. 11.2 Key considerations Below a summary of the general considerations are listed in critical appraisal of a ML research article. Issues to consider Details Clinical utility Explanation or rationale of why these prediction models are being built or developed? Was the study aim clear? Is it prediction of outcome, or identification important features? Data source and study description How was the data collected? What was the study design? RCT, observational, cross-sectional, longitudinal, nationally representative survey? Study start, end dates reported? What was the baseline? Are the data measurable in clinical setting routinely or they are measured irregularly? Target population Was it clear who was the target population where this model was developed and where it can be generalized? Analytic data How was the data pre-processed? Inclusion, exclusion criteria properly implemented to properly target the intended population? Clinicians were consulted to discuss the appropriateness of inclusion, exclusion criteria? Protocol published a priori? Data dimension, and split ratio Total data size, analytic data size, training, tuning, testing data size? Outcome label How was the gold standard determined, and what was the quality? The prediction of such outcome clinically relevant? Features How many covariates or features used? How were these variables selected? Subject area experts consulted in selection and identification of some or all of these variables? Any of these variables transformed or dichotomized or categorized or combined? A table of baseline characteristics of the subjects, stratified by the outcome labels presented? Missing data Were the amount of missing observations reported? Any explanation of why they were missing? How were the missing values handles? Complete case or multiple imputation? ML model choice Rationale of the ML model choice (logistic, LASSO, CART or extensions, ensemble, or others)? Model specification? Additive, linear or not? Amount of data adequate given the model complexity (number of parameters)? ML model details Details about ML model and implementation reported? Model fine tuned? Model somehow customized? Hyperparameters provided? Optimism or overfitting What method was used to address these issues? What measures of performances were used? Was there any performance gap (between tuned model vs internal validation model)? Model performance reasonable, or unrealistic? Generalizability External validation data present? Model was tested in real-world clinical setting? Reproducibility repeatable and reproducible? These can be in 3 levels (i) model (ii) code (iii) data or their combinations. Software code provided? Which software and version was used? Was the computing time reported? Interpretability Clinicians were consulted? Results were interpreted in collaboration with clinicians and subject area experts? Model results believable, interpretable? Subgroup Clinically important subgroups considered? 11.3 Example Download the article by Antman et al. (2000) (link). Try to identify how many of the above key considerations they have reported in the process of developing a risk score? OpenSafely article in Nature: lets discuss research goal. BIGGEST TABLE 2 FALLACY EVER#EpiTwitter, do you remember the discussions around the @OpenSafely Williamson @nature study this summer?The French @gouvernementFR admitted they used this study&#39;s Table 2 to withdraw protections from 100.000s of workers at risk of severe #COVID19! pic.twitter.com/qpllRe90N7&mdash; Olivier Berruyer (@OBerruyer) October 12, 2020 11.4 Exercise Find an article in the medical literature (published in a peer-reviewed journal, could be related to the area that you work on, or are interested in) that used a machine learning method to build a clinical prediction model (here is an example). Critically appraise that article. References "],["references.html", "References", " References "]]
